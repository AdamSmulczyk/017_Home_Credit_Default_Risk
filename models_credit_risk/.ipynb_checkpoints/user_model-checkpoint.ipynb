{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f05e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox, stats\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer, MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder \n",
    "from sklearn.model_selection import train_test_split, cross_val_score,StratifiedKFold, KFold, GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold, RepeatedKFold\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve, auc, roc_auc_score, cohen_kappa_score, accuracy_score, adjusted_mutual_info_score, mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, matthews_corrcoef, average_precision_score,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, VotingRegressor, VotingClassifier\n",
    "from sklearn.tree import  DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from collections import Counter\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "import optuna\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, Type\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import warnings\n",
    "# DATA VISUALIZATION\n",
    "# ------------------------------------------------------\n",
    "# import skimpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# CONFIGURATIONS\n",
    "# ------------------------------------------------------\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('float_format', '{:.3f}'.format)\n",
    "\n",
    "\n",
    "class Model(ABC):\n",
    "    \n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -> None:\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Predict probabilities for compatibility with multi-class scoring.\"\"\"\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit method for compatibility with sklearn's cross_val_score.\"\"\"\n",
    "        self.train(X, y)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y, scoring: str = 'accuracy'):\n",
    "\n",
    "        if scoring == 'accuracy':\n",
    "            # Use standard accuracy for classification.\n",
    "            predictions = self.predict(X)\n",
    "            return accuracy_score(y, predictions)\n",
    "        elif scoring == 'roc_auc_ovo':\n",
    "            # Use ROC AUC score for multi-class problems.\n",
    "            probabilities = self.predict_proba(X)\n",
    "            return roc_auc_score(y, probabilities, multi_class='ovo', average='macro')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scoring method: {scoring}. Supported metrics are 'accuracy' and 'roc_auc_ovo'.\")\n",
    "            \n",
    "class LGBModel(Model, BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"XGBoost Classifier model with extended parameter support.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = LGBMClassifier(**kwargs)            \n",
    "    \n",
    "class XGBoostModel(Model, BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"XGBoost Classifier model with extended parameter support.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = XGBClassifier(**kwargs)\n",
    "        \n",
    "class CatBoostModel(Model, BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"CatBoost Classifier model with extended parameter support.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = CatBoostClassifier(**kwargs)\n",
    "        \n",
    "class RandomForestModel(Model, BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Random Forest Classifier model.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = RandomForestClassifier(**kwargs)   \n",
    "\n",
    "class VotingModel(Model, BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Voting Classifier combining RFC_1 and XGB_2.\"\"\"\n",
    "    def __init__(self, estimators, voting='soft', weights=None):\n",
    "        self.estimators = estimators\n",
    "        self.voting = voting\n",
    "        self.weights = weights\n",
    "        self.model = VotingClassifier(estimators=self.estimators, voting=self.voting, weights=self.weights)     \n",
    "        \n",
    "class ModelFactory:\n",
    "    \"\"\"Factory to create model instances.\"\"\"\n",
    "    @staticmethod\n",
    "    def get_model(model_name: str, **kwargs) -> Model:\n",
    "        model_class = globals()[model_name]\n",
    "        return model_class(**kwargs)\n",
    "\n",
    "class Workflow_6:\n",
    "    \"\"\"Main workflow class for model training and evaluation.\"\"\"\n",
    "\n",
    "    def run_workflow(self, \n",
    "                     model_name: str, \n",
    "                     model_kwargs: dict, \n",
    "                     X: pd.DataFrame, \n",
    "                     y: pd.Series,\n",
    "                     test_size: float, \n",
    "                     random_state: int,\n",
    "                     scoring: str = 'accuracy'\n",
    "                    ) -> None:\n",
    "        \"\"\"\n",
    "        Main entry point to run the workflow:\n",
    "        - Splits the data.\n",
    "        - Trains the model.\n",
    "        - Evaluates the model.\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = self.split_data(X, y, test_size, random_state)\n",
    "\n",
    "        if model_name == 'VotingModel':\n",
    "            model = VotingModel(**model_kwargs)\n",
    "        else:\n",
    "            model = ModelFactory.get_model(model_name, **model_kwargs)\n",
    "\n",
    "        model.train(X_train, y_train)\n",
    "        \n",
    "        results = self.evaluate_model(model, X_train, X_test, y_train, y_test, scoring)\n",
    "        \n",
    "        print(\"Model Evaluation Results:\")\n",
    "        print(results.to_string())     \n",
    "        \n",
    "        plot = self.evaluate_plots(model, X_test, y_test, model_name)\n",
    "\n",
    "    def split_data(self, \n",
    "                   X: pd.DataFrame, \n",
    "                   y: pd.Series, \n",
    "                   test_size: float, \n",
    "                   random_state: int\n",
    "                  ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        \"\"\"Split the data into train and test sets.\"\"\"\n",
    "        return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    def evaluate_model(self, \n",
    "                       model: Model, \n",
    "                       X_train: pd.DataFrame, \n",
    "                       X_test: pd.DataFrame, \n",
    "                       y_train: pd.Series, \n",
    "                       y_test: pd.Series,\n",
    "                       scoring: str) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"\n",
    "        Evaluate the model using custom metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        - model: Trained model to evaluate.\n",
    "        - X_train, X_test: Feature datasets for training and testing.\n",
    "        - y_train, y_test: Target datasets for training and testing.\n",
    "        \n",
    "        Returns:\n",
    "        - pd.DataFrame: DataFrame containing evaluation metrics for train and test sets.\n",
    "        \"\"\"\n",
    "        def compute_metrics(y_true: pd.Series, y_pred_proba: np.ndarray) -> pd.Series:\n",
    "            \"\"\"Helper function to calculate metrics.\"\"\"\n",
    "            cutoff = np.sort(y_pred_proba)[-y_true.sum():].min()\n",
    "            y_pred_class = np.array([1 if x >= cutoff else 0 for x in y_pred_proba])\n",
    "        \n",
    "            \"\"\"Evaluate the model and return evaluation metrics as a dictionary.\"\"\"\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "            \"\"\"Evaluate the model using Stratified K-Fold cross-validation.\"\"\"\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            n_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring=scoring)   \n",
    "        \n",
    "            return pd.Series({\n",
    "                'F1_score': round(f1_score(y_true, y_pred_class), 4),\n",
    "                'P-R_score': round(average_precision_score(y_true, y_pred_class), 4), #Precision-Recall\n",
    "                'Matthews': round(matthews_corrcoef(y_true, y_pred_class), 4),  #the Matthews correlation coefficient (MCC)\n",
    "                'Accuracy': round(accuracy_score(y_true, y_pred_class), 4),\n",
    "                'Recall': round(recall_score(y_true, y_pred_class), 4),\n",
    "                'Precision': round(precision_score(y_true, y_pred_class), 4), \n",
    "                'SKF': np.mean(n_scores),                                        #Stratified K-Fold\n",
    "                'AUC': roc_auc_score(y_true, y_pred_class),                      #the Area Under the Curve\n",
    "                'Min_cutoff': cutoff,\n",
    "            })\n",
    "        \n",
    "        train_metrics = compute_metrics(y_train, model.predict_proba(X_train)[:, 1])\n",
    "        test_metrics = compute_metrics(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        \n",
    "        return pd.DataFrame({'TRAIN': train_metrics, 'TEST': test_metrics}).T\n",
    "\n",
    "    def evaluate_plots(self, \n",
    "                       model: Model,  \n",
    "                       X_test: pd.DataFrame,  \n",
    "                       y_test: pd.Series,\n",
    "                       model_name: str):\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if len(model.predict_proba(X_test).shape) > 1 else model.predict_proba(X_test)\n",
    "#         cutoff = np.sort(y_pred_proba)[-y_test.sum():].min()\n",
    "        cutoff=0.2\n",
    "        y_pred_class = np.array([1 if x >= cutoff else 0 for x in y_pred_proba])\n",
    "    \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "        # Plot ROC curve\n",
    "        roc_auc_test_cat = roc_auc_score(y_test, y_pred_proba)\n",
    "        fpr_cat, tpr_cat, _ = roc_curve(y_test, y_pred_proba)\n",
    "        axes[0].plot(fpr_cat, tpr_cat, label=f'ROC AUC = {roc_auc_test_cat:.4f}')\n",
    "        axes[0].plot([0, 1], [0, 1], 'k--')\n",
    "        axes[0].set_xlabel('False Positive Rate')\n",
    "        axes[0].set_ylabel('True Positive Rate')\n",
    "        axes[0].set_title(f'Confusion Matrix - threshold = {cutoff:.3f}')\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm_cat = confusion_matrix(y_test, predictions)\n",
    "\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        sns.heatmap(cm_cat, annot=True, fmt='.0f', cmap='viridis', cbar=False, ax=axes[1])\n",
    "        axes[1].set_xlabel('Predicted labels')\n",
    "        axes[1].set_ylabel('True labels')\n",
    "        axes[1].set_title(f'Confusion Matrix {model_name}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
